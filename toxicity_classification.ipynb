{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unintended Bias in Toxicity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Final Project for LING-583, Spring 2019*\n",
    "\n",
    "*Bryan D. Hayes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the incomprehensible volume of traffic on internet discussion forums and the propensity for forum users to behave uncivilly towards one another, there is a demand for automated detectors of toxic behavior. While simple classifier prove quite effective at determining when toxic behavior is taking place, these classifer can become biased against certain identity groups because the names of these groups tend to be invoked in toxic comments. The word \"gay\", for example, can be used as a description of identity in a toxic comment and as an insult in a non-toxic comment. As a result, the word \"gay\" may be an informative feature for a classifier in deciding that a comment is toxic, flagging non-toxic comments containing the word as toxic in the process.\n",
    "\n",
    "Our goal is to build a simple classifier, measure its bias, and then attempt to reduce the bias against certain identity groups by separating toxic uses of an identity label from non-toxic uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cytoolz import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this task is taken from the Jigsaw Unintended Bias in Toxicity Classification competition.\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification\n",
    "\n",
    "The provided dataset contains 1.8 million comments; to ease computation, we will sample just 100,000 of them for training and 10,000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\").sample(110000, random_state = 583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides the fraction of annotators who believed each comment is toxic. To simplify our task, we will mark all comments with a target score of 0.5 or greater as toxic and all others as nontoxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['toxic'] = [(score >= 0.5) for score in data['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use spaCy to process the raw comment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['doc'] = list(nlp.pipe(data['comment_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separate our training data from our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data,\n",
    "                               train_size = 100000, test_size = 10000,\n",
    "                               stratify = data['toxic'],\n",
    "                               random_state = 583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "False    91988\n",
       "True      8012\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('toxic').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that about 92% of comments are nontoxic, so we expect a dummy classifier that predicts all comments to be nontoxic will perform quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.dummy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer=<cyfunction identity at 0x0000026CEC250C80>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), p...), ('dummyclassifier', DummyClassifier(constant=None, random_state=None, strategy='most_frequent'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = make_pipeline(CountVectorizer(analyzer = identity), DummyClassifier('most_frequent'))\n",
    "baseline.fit(train['comment_text'], train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.99000000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.score(test['comment_text'], test['toxic']) * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a simple logistic regression classifier that will classify comments based on their tokenized representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def tokens(doc):\n",
    "    return [tok.lower_ for tok in doc]\n",
    "\n",
    "train['tokens'] = train['doc'].apply(tokens)\n",
    "test['tokens'] = test['doc'].apply(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer=<cyfunction identity at 0x000001AE1CA74608>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), p...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_lr = make_pipeline(CountVectorizer(analyzer = identity),\n",
    "                            LogisticRegression(solver ='liblinear', max_iter = 500))\n",
    "baseline_lr.fit(train['tokens'], train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_lr.score(test['tokens'], test['toxic']) * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier improves on the dummy classifier by about 2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further improve our classifier's score by selecting optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {'logisticregression__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "         'countvectorizer__min_df':[1, 2, 5],\n",
    "         'countvectorizer__max_df':[0.5, 0.75, 0.9]}\n",
    "grid = GridSearchCV(baseline_lr, params, n_jobs=-1, cv=3)\n",
    "grid.fit(train['tokens'], train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer=<cyfunction identity at 0x0000026CEC250C80>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), p...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'logisticregression__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'countvectorizer__min_df': [1, 2, 5], 'countvectorizer__max_df': [0.5, 0.75, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(train['tokens'], train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__max_df': 0.5,\n",
       " 'countvectorizer__min_df': 1,\n",
       " 'logisticregression__C': 1.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_lr.set_params(**grid.best_params_)\n",
    "baseline_lr.fit(train['tokens'], train['toxic'])\n",
    "baseline_lr.score(test['tokens'], test['toxic']) * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a classifier that is rather successful at predicting the toxicity of a comment, but is it biased? To measure this, we will look at three ROC-AUC metrics:\n",
    "\n",
    "- Subgroup: the performance of the classifier only on comments mentioning an identity subgroup.\n",
    "- Background-positive, subgroup-negative: the performance of the classifier on non-toxic comments mentioning the identity and toxic comments that don't mention the identity.\n",
    "- Background-negative, subgroup-positive: the performance of the classifier on toxic comments mentioning the identity and non-toxic comments that don't mention the identity.\n",
    "\n",
    "The identity subgroups considered are those that are present in more than 500 comments in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "identities = [\"male\", \"female\", \"homosexual_gay_or_lesbian\", \"christian\", \"jewish\", \"muslim\", \"black\", \"white\", \"psychiatric_or_mental_illness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgroup_auc(group_label, model, col_label = 'tokens'):\n",
    "    subgroup_test_set = test[test[group_label] > 0]\n",
    "    score = roc_auc_score(subgroup_test_set['toxic'], model.predict_proba(subgroup_test_set[col_label])[:,1])\n",
    "    print(\"Subgroup | \" + group_label + \": \" + str(score))\n",
    "    return score\n",
    "    \n",
    "def bpsn_auc(group_label, model, col_label = 'tokens'):\n",
    "    subgroup_test_set = test[((test[group_label] > 0) & (test['toxic'] == False)) | \n",
    "                             ((test[group_label] == 0) & (test['toxic'] == True))]\n",
    "    score = roc_auc_score(subgroup_test_set['toxic'], model.predict_proba(subgroup_test_set[col_label])[:,1])\n",
    "    print(\"BPSN | \" + group_label + \": \" + str(score))\n",
    "    return score\n",
    "    \n",
    "def bnsp_auc(group_label, model, col_label = 'tokens'):\n",
    "    subgroup_test_set = test[((test[group_label] > 0) & (test['toxic'] == True)) | \n",
    "                             ((test[group_label] == 0) & (test['toxic'] == False))]\n",
    "    score = roc_auc_score(subgroup_test_set['toxic'], model.predict_proba(subgroup_test_set[col_label])[:,1])\n",
    "    print(\"BNSP | \" + group_label + \": \" + str(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare bias across subgroups, we use a power mean function to more heavily penalize the model for its poorest-performing subgroup classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_auc(group_labels, model, metric_func, col_label = 'tokens', p = -5):\n",
    "    subgroup_scores = [(metric_func(label, model, col_label) ** p) for label in group_labels]\n",
    "    mean_auc = (np.mean(subgroup_scores)) ** (1 / p)\n",
    "    return mean_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overall sense of the model's bias, we average the overall ROC-AUC score as well as the three submetric scores defind above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_score(group_labels, model, metric_funcs, weights, col_label = 'tokens', p = -5):\n",
    "    score = weights[0] * roc_auc_score(test['toxic'], model.predict_proba(test[col_label])[:,1])\n",
    "    for i in range(0, len(metric_funcs)):\n",
    "        score += (weights[i + 1] * mean_auc(group_labels, model, metric_funcs[i], col_label, p))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now measure the bias in our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup | male: 0.8213306215524945\n",
      "Subgroup | female: 0.8435256151674062\n",
      "Subgroup | homosexual_gay_or_lesbian: 0.7009803921568627\n",
      "Subgroup | christian: 0.8856655290102389\n",
      "Subgroup | jewish: 0.860655737704918\n",
      "Subgroup | muslim: 0.8048279404211608\n",
      "Subgroup | black: 0.7920751633986928\n",
      "Subgroup | white: 0.7904761904761904\n",
      "Subgroup | psychiatric_or_mental_illness: 0.8424479166666666\n",
      "BPSN | male: 0.8374824460615345\n",
      "BPSN | female: 0.8291709767991725\n",
      "BPSN | homosexual_gay_or_lesbian: 0.8013840830449827\n",
      "BPSN | christian: 0.8764232081911264\n",
      "BPSN | jewish: 0.8486665035478346\n",
      "BPSN | muslim: 0.7944479319243916\n",
      "BPSN | black: 0.7986685032139578\n",
      "BPSN | white: 0.7821428571428573\n",
      "BPSN | psychiatric_or_mental_illness: 0.799599358974359\n",
      "BNSP | male: 0.8326326413546227\n",
      "BNSP | female: 0.8545525784884205\n",
      "BNSP | homosexual_gay_or_lesbian: 0.7501765536723163\n",
      "BNSP | christian: 0.8383829039271011\n",
      "BNSP | jewish: 0.8562005277044855\n",
      "BNSP | muslim: 0.8619052329607281\n",
      "BNSP | black: 0.8258398900961659\n",
      "BNSP | white: 0.8446104074889867\n",
      "BNSP | psychiatric_or_mental_illness: 0.8953747379454927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8327445661725307"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_score(identities, baseline_lr, [subgroup_auc, bpsn_auc, bnsp_auc], [0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that bias is not evenly distributed. The model is particularly biased against the homosexual_gay_or_lesbian subgroup; we see that the BNSP score for this subgroup is very low, indicating that the classifier tends to associate comments about this subgroup with toxicity.\n",
    "\n",
    "The below comment, for example, was rated toxic by only 1 of 5 annotators, but was determined by the model to be toxic with 76% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sounds like typical liberal, resorting to name calling, if anyone is a bigot its you against Christians,  I'm not a Christian or a bigot, I do have several gay friends that are really embarrassed about gay pride parade and all of the fuss,  they don't want it and have a good life,  its the liberal agenda pushing it,  if you think anyone will change their minds by the government saying it will be so,  think again.\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[107433]['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we might reduce bias in the model is by distinguishing references to an identity group used in a toxic context from those used in a non-toxic context. We could accomplish this in part by tagging any words used in conjunction with a recognized profanity. This would allow the classifier to separate normal uses of a term from those used in a clearly insulting context.\n",
    "\n",
    "We will use Google's list of profanities, obtained from https://github.com/RobertJGabriel/Google-profanity-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bad_words.txt\", \"r\") as f:\n",
    "    bad_words = []\n",
    "    for line in f:\n",
    "        bad_words.append(line.strip(\"\\n\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_doc = list(nlp.pipe(bad_words))\n",
    "bad_tokens = [word[0] for word in bad_doc]\n",
    "bad_strings = [tok.text for tok in bad_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('profane', default=False, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we will look for words that are modified by a recognized profanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in data['doc']:\n",
    "    for tok in doc:\n",
    "        if tok.text in bad_strings and tok.dep_ == 'amod':\n",
    "            tok.head._.profane = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now modify tokens used in a profane context, adding in additional dependency relationships along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profane(tok):\n",
    "    return 'PROFANE:' + tok.lower_ if tok._.profane else tok.lower_\n",
    "\n",
    "def everything(doc):\n",
    "    return [profane(w.head) + '_' + profane(w) for w in doc if w.head != w ] + \\\n",
    "           [profane(w) for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['everything'] = train['doc'].apply(everything)\n",
    "test['everything'] = test['doc'].apply(everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.13"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_lr.fit(train['everything'], train['toxic'])\n",
    "baseline_lr.score(test['everything'], test['toxic']) * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this method of tagging the data provides a small increase in overall performance. How was bias affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup | male: 0.8344581060676096\n",
      "Subgroup | female: 0.8609116579265834\n",
      "Subgroup | homosexual_gay_or_lesbian: 0.7331932773109244\n",
      "Subgroup | christian: 0.9176949330532949\n",
      "Subgroup | jewish: 0.8319672131147541\n",
      "Subgroup | muslim: 0.8061119671289163\n",
      "Subgroup | black: 0.809640522875817\n",
      "Subgroup | white: 0.8184523809523809\n",
      "Subgroup | psychiatric_or_mental_illness: 0.8841145833333334\n",
      "BPSN | male: 0.8392697561598366\n",
      "BPSN | female: 0.8373282104329836\n",
      "BPSN | homosexual_gay_or_lesbian: 0.8241637831603229\n",
      "BPSN | christian: 0.8921638225255972\n",
      "BPSN | jewish: 0.8494005382921458\n",
      "BPSN | muslim: 0.8109437120736556\n",
      "BPSN | black: 0.8068755739210286\n",
      "BPSN | white: 0.7704573934837092\n",
      "BPSN | psychiatric_or_mental_illness: 0.8166666666666667\n",
      "BNSP | male: 0.8551346562978777\n",
      "BNSP | female: 0.8751604781833037\n",
      "BNSP | homosexual_gay_or_lesbian: 0.7750453995157385\n",
      "BNSP | christian: 0.8701142513529765\n",
      "BNSP | jewish: 0.8533641160949867\n",
      "BNSP | muslim: 0.8574537540805223\n",
      "BNSP | black: 0.8470557012613962\n",
      "BNSP | white: 0.8917377936857561\n",
      "BNSP | psychiatric_or_mental_illness: 0.9013692348008385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8465474409689075"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_score(identities, baseline_lr, [subgroup_auc, bpsn_auc, bnsp_auc], [0.25, 0.25, 0.25, 0.25], col_label = 'everything')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one transformation was able to increase our overall bias score by almost 2%. Clearly, a substantial amount of bias remains in the model, and more sophisticated techniques for recognizing toxicity would be necessary to eliminate it. However, we hopefully have shown that it is possible to reduce model bias, so unbiased automated toxicity detection may someday be a reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
